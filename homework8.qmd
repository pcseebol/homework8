---
title: "Homework8"
author: "Patrick Seebold"
format: pdf
editor: visual
---

This homework will explore basics of predictive modeling using an online-available data set concerning bike data in Seoul, South Korea.

First, let's get our libraries set:

```{r, echo = FALSE}
library(tidymodels)
library(lubridate)
library(dplyr)
library(ggplot2)
```

## EDA

Next, let's load the data in and resolve the known error mentioned in the instructions. We can specify the fileEncoding and avoid checking names to solve the problem:

```{r}
data = read.csv("SeoulBikeData.csv", fileEncoding = "Latin1", check.names = F)
```

Great, now we can go ahead with our EDA to make sure everything looks good. First, lets check for missing values:

```{r}
sum(is.na(data))
```

Sweet, no missing values. makes for a good start. Next, we'll take a look at the column types, rename to avoid strange variable names, and change our categorical variables to factors. The instructions say to change our variable names later in the process, but I'd rather they be in a clean format for the rest of the EDA!

```{r}
summary(data)
colnames(data) = c('Date', 'NumBikes', "Hour", "Temperature", "Humidity",
                   "WindSpeed","Visibility", "DewPointTemp", "SolarRad", "Rainfall"
                   , "Snowfall", "Season", "Holiday", "FunctioningDay" )

data$Season = as.factor(data$Season)
data$Holiday = as.factor(data$Holiday)
data$FunctioningDay = as.factor(data$FunctioningDay)
levels(data$Season)
levels(data$Holiday)
levels(data$FunctioningDay)
```

Great, now we can see that everything looks good with our numerical variables, our variable names are no longer likely to cause an issue, and our categorical variables are appropriately recast as factors! Next, we will change the date into a workable arithmetic form using lubridate:

```{r}
typeof(data$Date) # currently character

data$Date = lubridate::dmy(data$Date)
typeof(data$Date) # now it's a double!
```

Now that we have cleaned the data up, we will do some summary stats, specifically looking at bike rental count, rainfall, and snowfall. We'll also examine these across some categorical variables, such as FunctioningDay, Holiday, and Season:

```{r}
data |> # check bike numbers by season
  group_by(FunctioningDay) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))# Non-functioning days mean no bikes!

sum(data$FunctioningDay == "No") # 295 of the data points can be excluded

sub_data = subset(data, data$FunctioningDay == "Yes") # we can ignore days that are not functioning


sub_data |> # check bike numbers by season
  group_by(Season) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))

sub_data |> # check bike numbers by holiday
  group_by(Holiday) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))

sub_data |> # check rain  by season
  group_by(Season) |>
  summarize(mean = mean(Rainfall),  sd = sd(Rainfall))

sub_data |> # check snowfall by season
  group_by(Season) |>
  summarize(mean = mean(Snowfall),  sd = sd(Snowfall))
```

We see that summer appears to have the highest number of bike rentals, and winter has the fewest. Bike rentals are more common on Non-Holidays, so they are likely being used to commute to work. Finally, we see from our tables that snowfall is most plentiful in winter, while rainfall is most plentiful in summer.

Next up, let's summarize across hours so that we can collapse each day into a single observation.

```{r}
clean_data = sub_data |>
  group_by(Date, Season, Holiday) |>
  summarize(TotBikes = sum(NumBikes), TotRain = sum(Rainfall), TotSnow = sum(Snowfall), MeanTemp = mean(Temperature), MeanHumid = mean(Humidity), MeanWindSpeed = mean(WindSpeed), MeanVis = mean(Visibility), MeanDewPoint = mean(DewPointTemp), MeanSolar = mean(SolarRad))
head(clean_data)
  
```

Great, now we have our final data set for training. Let's recreate our basic summaries and do some plots on this cleaned data. We'll also report some correlations. Let's first recreate our summary stats:

```{r}

clean_data |> # check bike numbers by season
  group_by(Season) |>
  summarize(mean = mean(TotBikes), sd = sd(TotBikes))

clean_data |> # check bike numbers by holiday
  group_by(Holiday) |>
  summarize(mean = mean(TotBikes), sd = sd(TotBikes))

clean_data |> # check rain  by season
  group_by(Season) |>
  summarize(mean = mean(TotRain),  sd = sd(TotRain))

clean_data |> # check snowfall by season
  group_by(Season) |>
  summarize(mean = mean(TotSnow),  sd = sd(TotSnow))
```

We see that the same trends hold; more bikes in summer, more bikes on non-holidays, more rain in summer, and more snow in winter. Next, let's plot some of the numerical variables:

```{r}
ggplot(clean_data, aes(x = TotRain, y = TotBikes)) +
       geom_point() +
       labs(x = 'Total Rain', y = 'Total Bikes', title = 'Bike Rentals by Amount of rain')

ggplot(clean_data, aes(x = TotSnow, y = TotBikes)) +
       geom_point() +
       labs(x = 'Total Snow', y = 'Total Bikes', title = 'Bike Rentals by Amount of snow')

ggplot(clean_data, aes(x = MeanTemp, y = TotBikes)) +
       geom_point() +
       geom_smooth() +
       labs(x = 'Mean Temperature', y = 'Total Bikes', title = 'Bike Rentals by Temperature')
```

These plots make sense. We see a lot more bike rentals when there is low snow and low rain. Also, we see that the number of bikes increase as temperature increases, until a certain point. These plots appear to be accurately capturing info about how weather influence bike riding behavior!

Finally, let's plot some correlations:

```{r}
cor(clean_data[sapply(clean_data, is.numeric)])
```

We can see some impressive correlations here. For example, Mean Temp, Mean Dew Point, and Mean Solar are all quite positively correlated with total number of bike rentals. These are likely all tied into the weather we commented on above - when the weather is nice, people are more likely to rent bikes, thus leading to higher correlation values! We should be able to do some nice prediction on this data given the strengths of these correlations, although we might be at risk of overfitting if we aren't careful.

## Modeling the Data

Now we can prep our model for prediction! First, let's make our test and training sets:

```{r}
set.seed(42)
splits = initial_split(clean_data, prop = 0.75, strata = "Season")
train = training(splits)
test = testing(splits)

head(train)
nrow(train)
head(test)
nrow(test)
```

Everything appears to be in order with our test and training sets, and we have stratified by Season as requested. Next up, we'll make three recipes, train our three MLR models, and then compare their performances:

```{r}
# All three of our recipes will use the same recipes. However, model 1 will only include all variables as predictors, while models 2 and 3 will include more complex terms.

# Recipe 1: all numeric variables, no interactions
bike_rec1 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) #|>
  #prep(training = clean_data) |>
  #bake(clean_data) # let's make sure it worked as planned!

# Recipe 2: all numeric variables, some interactions
bike_rec2 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) |>
  step_interact(~ Season_Spring:Holiday_No.Holiday +
                  Season_Summer:Holiday_No.Holiday +
                  Season_Winter:Holiday_No.Holiday +
                  Season_Spring:TotRain + Season_Summer:TotRain +
                  Season_Winter:TotRain + 
                  MeanTemp:TotRain)

# Recipe 3: all numeric variables, some interacitons, & quad term
bike_rec3 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) |>
  step_interact(~ Season_Spring:Holiday_No.Holiday +
                  Season_Summer:Holiday_No.Holiday +
                  Season_Winter:Holiday_No.Holiday +
                  Season_Spring:TotRain + Season_Summer:TotRain +
                  Season_Winter:TotRain + 
                  MeanTemp:TotRain) |>
  step_poly(TotRain, TotSnow, MeanTemp, MeanHumid, MeanWindSpeed,
            MeanVis, MeanDewPoint , MeanSolar, degree = 2)
```

After prepping and baking the data, we can see that everything seems to be in order! Now, we can declare our model and set up our workflow:

```{r}
bike_mod = linear_reg() %>%
  set_engine("lm")

bike_wfl1 = workflow() |>
  add_recipe(bike_rec1) |>
  add_model(bike_mod)
             
bike_wfl2 = workflow() |>
  add_recipe(bike_rec2) |>
  add_model(bike_mod)

bike_wfl3 = workflow() |>
  add_recipe(bike_rec3) |>
  add_model(bike_mod)
```

Cool, now we have the workflows for each of our models ready to! Let's set up the 10-fold cross validation next:

```{r}
bike_cv10 = vfold_cv(clean_data, 10) # same CV for all models

bike_cv_fits1 = bike_wfl1 |>
  fit_resamples(bike_cv10)
bike_cv_fits1 |>
  collect_metrics()

bike_cv_fits2 = bike_wfl2 |>
  fit_resamples(bike_cv10)
bike_cv_fits2 |>
  collect_metrics()

bike_cv_fits3 = bike_wfl3 |>
  fit_resamples(bike_cv10)
bike_cv_fits3 |>
  collect_metrics()
```

After fitting all three models, we see that the first one has the lowest RMSE. Also, this first model is the simplest of the set! We will use recipe 1 on the full training set and then use collect_metrics() to find the test RMSE:

```{r}
final_mod = last_fit(bike_wfl1, split = splits)
clean_final_mod = collect_metrics(final_mod)
clean_final_mod
```

And finally, we can obtain the final model coefficient table:

```{r}
final_fits = extract_fit_parsnip(final_mod)
final_coef = tidy(final_fits)
final_coef
```

And there we have it! We can see how the various different parameters are expected to impact bike rentals in this specific town, and could use these to predict how well our bikes may fair depending on the weather, season, and other variables.
